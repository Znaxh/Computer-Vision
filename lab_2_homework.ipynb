{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyTtiOK4HKK"
      },
      "source": [
        "# **Home Assignment 1:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8chifvcu4bh",
        "outputId": "461e4b66-8272-4bd0-ac3c-eb77fb566fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.7.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n",
            "Generating record list for: 100\n",
            "Generating record list for: 101\n",
            "Generating record list for: 102\n",
            "Generating record list for: 103\n",
            "Generating record list for: 104\n",
            "Generating record list for: 105\n",
            "Generating record list for: 106\n",
            "Generating record list for: 107\n",
            "Generating record list for: 108\n",
            "Generating record list for: 109\n",
            "Generating record list for: 111\n",
            "Generating record list for: 112\n",
            "Generating record list for: 113\n",
            "Generating record list for: 114\n",
            "Generating record list for: 115\n",
            "Generating record list for: 116\n",
            "Generating record list for: 117\n",
            "Generating record list for: 118\n",
            "Generating record list for: 119\n",
            "Generating record list for: 121\n",
            "Generating record list for: 122\n",
            "Generating record list for: 123\n",
            "Generating record list for: 124\n",
            "Generating record list for: 200\n",
            "Generating record list for: 201\n",
            "Generating record list for: 202\n",
            "Generating record list for: 203\n",
            "Generating record list for: 205\n",
            "Generating record list for: 207\n",
            "Generating record list for: 208\n",
            "Generating record list for: 209\n",
            "Generating record list for: 210\n",
            "Generating record list for: 212\n",
            "Generating record list for: 213\n",
            "Generating record list for: 214\n",
            "Generating record list for: 215\n",
            "Generating record list for: 217\n",
            "Generating record list for: 219\n",
            "Generating record list for: 220\n",
            "Generating record list for: 221\n",
            "Generating record list for: 222\n",
            "Generating record list for: 223\n",
            "Generating record list for: 228\n",
            "Generating record list for: 230\n",
            "Generating record list for: 231\n",
            "Generating record list for: 232\n",
            "Generating record list for: 233\n",
            "Generating record list for: 234\n",
            "Generating list of all files for: 100\n",
            "Generating list of all files for: 101\n",
            "Generating list of all files for: 102\n",
            "Generating list of all files for: 103\n",
            "Generating list of all files for: 104\n",
            "Generating list of all files for: 105\n",
            "Generating list of all files for: 106\n",
            "Generating list of all files for: 107\n",
            "Generating list of all files for: 108\n",
            "Generating list of all files for: 109\n",
            "Generating list of all files for: 111\n",
            "Generating list of all files for: 112\n",
            "Generating list of all files for: 113\n",
            "Generating list of all files for: 114\n",
            "Generating list of all files for: 115\n",
            "Generating list of all files for: 116\n",
            "Generating list of all files for: 117\n",
            "Generating list of all files for: 118\n",
            "Generating list of all files for: 119\n",
            "Generating list of all files for: 121\n",
            "Generating list of all files for: 122\n",
            "Generating list of all files for: 123\n",
            "Generating list of all files for: 124\n",
            "Generating list of all files for: 200\n",
            "Generating list of all files for: 201\n",
            "Generating list of all files for: 202\n",
            "Generating list of all files for: 203\n",
            "Generating list of all files for: 205\n",
            "Generating list of all files for: 207\n",
            "Generating list of all files for: 208\n",
            "Generating list of all files for: 209\n",
            "Generating list of all files for: 210\n",
            "Generating list of all files for: 212\n",
            "Generating list of all files for: 213\n",
            "Generating list of all files for: 214\n",
            "Generating list of all files for: 215\n",
            "Generating list of all files for: 217\n",
            "Generating list of all files for: 219\n",
            "Generating list of all files for: 220\n",
            "Generating list of all files for: 221\n",
            "Generating list of all files for: 222\n",
            "Generating list of all files for: 223\n",
            "Generating list of all files for: 228\n",
            "Generating list of all files for: 230\n",
            "Generating list of all files for: 231\n",
            "Generating list of all files for: 232\n",
            "Generating list of all files for: 233\n",
            "Generating list of all files for: 234\n",
            "Created local base download directory: mitdb\n",
            "Downloading files...\n",
            "Finished downloading files\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "import wfdb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Download the MIT-BIH Arrhythmia Database\n",
        "wfdb.dl_database('mitdb', dl_dir='mitdb')\n",
        "\n",
        "# Load the dataset\n",
        "record = wfdb.rdrecord('mitdb/100')\n",
        "annotation = wfdb.rdann('mitdb/100', 'atr')\n",
        "\n",
        "# Extract signals and corresponding annotations\n",
        "signals, fields = wfdb.rdsamp('mitdb/100')\n",
        "labels = np.zeros(signals.shape[0])  # Initialize labels with zeros\n",
        "\n",
        "# Create a mapping for the annotation symbols\n",
        "symbol_to_label = {'N': 0}  # Normal = 0\n",
        "# All other symbols will be considered abnormal (1)\n",
        "default_label = 1\n",
        "\n",
        "# Assign numeric labels based on the annotations\n",
        "for idx in range(len(annotation.sample)):\n",
        "    symbol = annotation.symbol[idx]\n",
        "    labels[annotation.sample[idx]] = symbol_to_label.get(symbol, default_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPVAlL1XwC4-"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: Normalize signals\n",
        "signals = (signals - np.mean(signals, axis=0)) / np.std(signals, axis=0)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(signals, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE-_qO0Kv0Qz"
      },
      "source": [
        "B. Create a Unimodal, Hybrid model, and Multimodal using given algorithms:\n",
        "• Machine learning\n",
        "• Deep learning\n",
        "• Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOIskgPv2fj"
      },
      "source": [
        "### 1)Unimodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IncGaJhKwGB_",
        "outputId": "9f100511-db21-4836-b37f-e146746db454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Model Accuracy: 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "# SVM Model\n",
        "svm_model = SVC(kernel='linear', C=1)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(f\"SVM Model Accuracy: {svm_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Z_n6l7wNAu"
      },
      "source": [
        "Deep Learning Model (Simple Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvtUQYFwK6_",
        "outputId": "bab00055-336f-4ace-9964-baa757a1b24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0091 - val_accuracy: 0.9999 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.4479e-04 - val_accuracy: 0.9999 - val_loss: 0.0044\n",
            "Epoch 3/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8208e-04 - val_accuracy: 0.9999 - val_loss: 0.0041\n",
            "Epoch 4/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.9325e-04 - val_accuracy: 0.9999 - val_loss: 0.0049\n",
            "Epoch 5/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.0436e-04 - val_accuracy: 0.9999 - val_loss: 0.0046\n",
            "Epoch 6/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.4789e-04 - val_accuracy: 0.9999 - val_loss: 0.0057\n",
            "Epoch 7/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.2385e-04 - val_accuracy: 0.9999 - val_loss: 0.0074\n",
            "Epoch 8/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.3679e-04 - val_accuracy: 0.9999 - val_loss: 0.0057\n",
            "Epoch 9/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.4043e-04 - val_accuracy: 0.9999 - val_loss: 0.0075\n",
            "Epoch 10/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.2585e-04 - val_accuracy: 0.9999 - val_loss: 0.0073\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.1440e-04\n",
            "Neural Network Accuracy: 0.9999461770057678\n"
          ]
        }
      ],
      "source": [
        "# Deep Learning Model\n",
        "nn_model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(signals.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test)\n",
        "print(f\"Neural Network Accuracy: {nn_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf4Cw9lQwWF8"
      },
      "source": [
        "Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bkq6U2YHySIM",
        "outputId": "cc53ad7f-68b8-48c2-9332-951b83a76c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(650000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(signals.shape)  # Should output something like (num_samples, height, width)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tY0dknLEykom",
        "outputId": "a783f9c5-a486-4bc3-c802-4145e55dc7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original signals shape: (650000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Original signals shape:\", signals.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ifd0dppAym2T",
        "outputId": "008914a5-a208-4c14-a984-c5ac69062f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signals shape: (650000, 2)\n"
          ]
        }
      ],
      "source": [
        "# If signals has shape (num_samples, num_features)\n",
        "# You may need to reshape or preprocess it to fit the expected format\n",
        "print(\"Signals shape:\", signals.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2CPsSJl9yqPN",
        "outputId": "0e8392a5-56d4-4734-d8a6-0f6f8357a79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original signals shape: (650000, 2)\n",
            "Total number of elements: 1300000\n"
          ]
        }
      ],
      "source": [
        "print(\"Original signals shape:\", signals.shape)\n",
        "print(\"Total number of elements:\", np.prod(signals.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l_3U3SXwy053",
        "outputId": "a84a8cda-3c36-42cf-d1e3-7a5f6b0941c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signals shape: (650000, 2)\n",
            "Cannot reshape data into the specified dimensions.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Print the shape to understand its structure\n",
        "print(\"Signals shape:\", signals.shape)\n",
        "\n",
        "# Example: suppose signals.shape is (num_samples, num_features)\n",
        "num_samples, num_features = signals.shape\n",
        "\n",
        "# Assume you have to reshape to (num_samples, height, width), where height and width should be inferred\n",
        "# You need to know or estimate height and width. Example: if num_features = height * width\n",
        "# For a hypothetical 64x64 image, num_features should be 4096 (64*64)\n",
        "height = 64  # Adjust based on your knowledge of the data\n",
        "width = num_features // height\n",
        "if height * width == num_features:\n",
        "    signals_reshaped = signals.reshape(num_samples, height, width)\n",
        "    signals_3ch = np.expand_dims(signals_reshaped, axis=-1)  # Add channel dimension\n",
        "    signals_3ch = np.repeat(signals_3ch, 3, axis=-1)  # Convert to 3 channels\n",
        "    print(\"Reshaped signals shape:\", signals_3ch.shape)\n",
        "else:\n",
        "    print(\"Cannot reshape data into the specified dimensions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ZZKm7KBzK8q",
        "outputId": "d426750e-e197-498a-ae59-4fc2e9abac3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of signals data: [[0.83488281 0.85035888]\n",
            " [0.83488281 0.85035888]\n",
            " [0.83488281 0.85035888]\n",
            " [0.83488281 0.85035888]\n",
            " [0.83488281 0.85035888]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample of signals data:\", signals[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CdAe-wPFzOoC",
        "outputId": "8dd069a9-a680-425a-ced7-b569c3642879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reshaped signals shape: (650000, 1, 2, 3)\n"
          ]
        }
      ],
      "source": [
        "# Hypothetical example for a specific case\n",
        "# Suppose each sample represents a 2D grid of size (height, width)\n",
        "# Here, height and width must be inferred correctly\n",
        "\n",
        "num_samples, num_features = signals.shape\n",
        "\n",
        "# Hypothetical example with assumed width and height for illustration\n",
        "# Adjust height and width based on actual understanding of your data\n",
        "height = 1  # As per your feature vector size\n",
        "width = 2  # As per your feature vector size\n",
        "\n",
        "if height * width == num_features:\n",
        "    signals_reshaped = signals.reshape(num_samples, height, width)\n",
        "    signals_3ch = np.expand_dims(signals_reshaped, axis=-1)  # Shape: (num_samples, height, width, 1)\n",
        "    signals_3ch = np.repeat(signals_3ch, 3, axis=-1)  # Shape: (num_samples, height, width, 3)\n",
        "    print(\"Reshaped signals shape:\", signals_3ch.shape)\n",
        "else:\n",
        "    print(\"Cannot reshape data into specified dimensions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6x0EJca_x-6B",
        "outputId": "a0acc6df-b959-49b3-a0dc-26bcda96b8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Model Accuracy: 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(signals, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM model\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Model Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LONFnIxfzyJD"
      },
      "source": [
        "### 1) Unimodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVc_GbQ0FcD"
      },
      "source": [
        "Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rWCXGQU1z8AD",
        "outputId": "d60c12f1-327e-4633-a971-ac70493eb2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Model Accuracy: 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(signals, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "y_pred = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Model Accuracy: {svm_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQwZ-bdS0IT3"
      },
      "source": [
        "Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eQKTFnqW0CmF",
        "outputId": "88643306-6c5f-4fb7-83b5-8f2ff7c5f945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0256 - val_accuracy: 0.9999 - val_loss: 0.0015\n",
            "Epoch 2/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.2324e-04 - val_accuracy: 0.9999 - val_loss: 0.0013\n",
            "Epoch 3/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.9867e-04 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 4/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.8560e-04 - val_accuracy: 0.9999 - val_loss: 0.0012\n",
            "Epoch 5/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.9904e-04 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 6/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.0321e-04 - val_accuracy: 0.9999 - val_loss: 0.0012\n",
            "Epoch 7/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.1164e-04 - val_accuracy: 0.9999 - val_loss: 0.0012\n",
            "Epoch 8/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.0102e-04 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 9/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.6409e-04 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 10/10\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.2352e-04 - val_accuracy: 0.9999 - val_loss: 0.0015\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.3258e-04\n",
            "Deep Learning Model Accuracy: 0.9999461770057678\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define and compile a simple neural network model\n",
        "dl_model = models.Sequential()\n",
        "dl_model.add(layers.Input(shape=(2,)))  # Assuming each sample has 2 features\n",
        "dl_model.add(layers.Dense(64, activation='relu'))\n",
        "dl_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "dl_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "dl_loss, dl_accuracy = dl_model.evaluate(X_test, y_test)\n",
        "print(f\"Deep Learning Model Accuracy: {dl_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uHQJL4PwbSa"
      },
      "source": [
        "### 2) Hybrid Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYKg_ZRM0asj"
      },
      "source": [
        "Combine SVM and Neural Network Predictions\n",
        "Train SVM and Neural Network Models\n",
        "\n",
        "Use Neural Network Predictions as Features for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "12KXmjE90gbA",
        "outputId": "ecba7c80-0221-4269-900d-6a5936665a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m16250/16250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1ms/step\n",
            "Shape of nn_predictions: (520000, 1)\n",
            "Shape of y_train: (520000,)\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n",
            "Shape of svm_nn_predictions: (130000,)\n",
            "Shape of y_test: (130000,)\n",
            "Hybrid Model Accuracy (SVM on NN Features): 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predictions from Neural Network\n",
        "nn_predictions = dl_model.predict(X_train)  # Get predictions from the deep learning model\n",
        "\n",
        "# Ensure that nn_predictions and y_train have the same number of samples\n",
        "print(f\"Shape of nn_predictions: {nn_predictions.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "\n",
        "# Split the data correctly\n",
        "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(nn_predictions, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a new SVM model using predictions from Neural Network as features\n",
        "svm_model_nn = SVC()\n",
        "svm_model_nn.fit(X_train_nn, y_train_nn)  # Note the use of y_train_nn here\n",
        "\n",
        "# Evaluate the Hybrid Model\n",
        "nn_test_predictions = dl_model.predict(X_test)  # Get predictions for the test set\n",
        "svm_nn_predictions = svm_model_nn.predict(nn_test_predictions)  # SVM model predictions\n",
        "\n",
        "# Ensure that the length of svm_nn_predictions matches y_test\n",
        "print(f\"Shape of svm_nn_predictions: {svm_nn_predictions.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "# Evaluate the accuracy\n",
        "svm_nn_accuracy = accuracy_score(y_test, svm_nn_predictions)\n",
        "print(f\"Hybrid Model Accuracy (SVM on NN Features): {svm_nn_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBTk_0Eu0jf-"
      },
      "source": [
        "### 3) Multimodal Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjypzfON0x67"
      },
      "source": [
        "Feature Augmentation\n",
        "Augment Features with Synthetic Data\n",
        "Add synthetic features to simulate multimodal input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Sguy7YY01YO",
        "outputId": "d05397d0-6902-47e9-f191-026e73b0d72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0218 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 2/5\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.4653e-04 - val_accuracy: 0.9999 - val_loss: 0.0013\n",
            "Epoch 3/5\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.4505e-04 - val_accuracy: 0.9999 - val_loss: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.5080e-04 - val_accuracy: 0.9999 - val_loss: 9.4303e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m14625/14625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.8446e-04 - val_accuracy: 0.9999 - val_loss: 9.2463e-04\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.1033e-04\n",
            "Multimodal Model Accuracy: 0.9999461770057678\n"
          ]
        }
      ],
      "source": [
        "# Create synthetic features\n",
        "synthetic_features = np.random.rand(signals.shape[0], 2)  # Example synthetic features\n",
        "\n",
        "# Combine original features with synthetic features\n",
        "combined_features = np.hstack((signals, synthetic_features))\n",
        "\n",
        "# Split and train with combined features\n",
        "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(combined_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate a model on combined features\n",
        "combined_model = models.Sequential()\n",
        "combined_model.add(layers.Input(shape=(4,)))  # Adjust input shape based on combined features\n",
        "combined_model.add(layers.Dense(64, activation='relu'))\n",
        "combined_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "combined_model.fit(X_train_combined, y_train_combined, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "combined_loss, combined_accuracy = combined_model.evaluate(X_test_combined, y_test_combined)\n",
        "print(f\"Multimodal Model Accuracy: {combined_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npJaPpVk12Jt"
      },
      "source": [
        "C. Evaluate the model performance:\n",
        " Split dataset into Train, Test, and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hjqGMX1h1zqX",
        "outputId": "94e68e6b-5030-4e09-881b-a32567b3980a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (416000, 2), Validation shape: (104000, 2), Test shape: (130000, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into Train + Validation and Test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(signals, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the Train + Validation set into Train and Validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_uzKC7kF14q5",
        "outputId": "470cc665-c772-4d97-dd60-bc606709ef38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Validation Accuracy: 0.999951923076923\n",
            "SVM Validation Precision: 0.0\n",
            "SVM Validation Recall: 0.0\n",
            "SVM Validation F1-Score: 0.0\n",
            "SVM Test Accuracy: 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_val_pred = svm_model.predict(X_val)\n",
        "svm_val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "svm_val_precision = precision_score(y_val, y_val_pred)\n",
        "svm_val_recall = recall_score(y_val, y_val_pred)\n",
        "svm_val_f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"SVM Validation Accuracy: {svm_val_accuracy}\")\n",
        "print(f\"SVM Validation Precision: {svm_val_precision}\")\n",
        "print(f\"SVM Validation Recall: {svm_val_recall}\")\n",
        "print(f\"SVM Validation F1-Score: {svm_val_f1}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = svm_model.predict(X_test)\n",
        "svm_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"SVM Test Accuracy: {svm_test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5-X1dXiS195y",
        "outputId": "1eb5b3ec-03b3-4e65-c91b-25b158605a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m13000/13000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0334 - val_accuracy: 1.0000 - val_loss: 4.1238e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m13000/13000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.0278e-04 - val_accuracy: 1.0000 - val_loss: 4.7839e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m13000/13000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.7621e-04 - val_accuracy: 1.0000 - val_loss: 5.0713e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m13000/13000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.8112e-04 - val_accuracy: 1.0000 - val_loss: 5.3493e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m13000/13000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.0269e-04 - val_accuracy: 1.0000 - val_loss: 4.9575e-04\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 8.9812e-04\n",
            "Deep Learning Validation Accuracy: 0.9999518990516663\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.7548e-04\n",
            "Deep Learning Test Accuracy: 0.9999461770057678\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define and compile the neural network model\n",
        "dl_model = models.Sequential()\n",
        "dl_model.add(layers.Input(shape=(2,)))  # Adjust shape based on the feature size\n",
        "dl_model.add(layers.Dense(64, activation='relu'))\n",
        "dl_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "dl_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate on the validation set\n",
        "val_loss, val_accuracy = dl_model.evaluate(X_val, y_val)\n",
        "print(f\"Deep Learning Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = dl_model.evaluate(X_test, y_test)\n",
        "print(f\"Deep Learning Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SiQDCHwr5zVT"
      },
      "outputs": [],
      "source": [
        "# Example image shape for VGG16\n",
        "input_shape = (224, 224, 3)  # 224x224 RGB images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Naop_teY51l5",
        "outputId": "7a2068e3-ddc4-4f79-af98-91a680ba8848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load pre-trained VGG16 model without top layer and freeze base layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "transfer_model = models.Model(inputs=base_model.input, outputs=predictions)\n",
        "transfer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqftWAIG2YD_"
      },
      "source": [
        "Accuracy, Loss, Precision, Recall, F1-score, Dice coefficient, and other parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ucUdPdx92azs",
        "outputId": "a20b59d1-7c55-4363-e7d4-6fd5e5e4e7a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Validation Accuracy: 0.999951923076923\n",
            "SVM Validation Precision: 0.0\n",
            "SVM Validation Recall: 0.0\n",
            "SVM Validation F1-Score: 0.0\n",
            "SVM Test Accuracy: 0.9999461538461538\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_val_pred = svm_model.predict(X_val)\n",
        "svm_val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "svm_val_precision = precision_score(y_val, y_val_pred)\n",
        "svm_val_recall = recall_score(y_val, y_val_pred)\n",
        "svm_val_f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"SVM Validation Accuracy: {svm_val_accuracy}\")\n",
        "print(f\"SVM Validation Precision: {svm_val_precision}\")\n",
        "print(f\"SVM Validation Recall: {svm_val_recall}\")\n",
        "print(f\"SVM Validation F1-Score: {svm_val_f1}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = svm_model.predict(X_test)\n",
        "svm_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"SVM Test Accuracy: {svm_test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LzYHISFu2dRR",
        "outputId": "02229f3c-0c68-40ee-af59-bead584580f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Validation Dice Coefficient: 0.0\n",
            "SVM Test Dice Coefficient: 0.0\n"
          ]
        }
      ],
      "source": [
        "def dice_coefficient(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    return 2. * intersection / (np.sum(y_true) + np.sum(y_pred))\n",
        "\n",
        "# Convert predictions to binary (if needed)\n",
        "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
        "dice_val = dice_coefficient(y_val, y_val_pred_binary)\n",
        "\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "dice_test = dice_coefficient(y_test, y_test_pred_binary)\n",
        "\n",
        "print(f\"SVM Validation Dice Coefficient: {dice_val}\")\n",
        "print(f\"SVM Test Dice Coefficient: {dice_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cgF1rBVD2fyn",
        "outputId": "1885e6a9-556c-4a89-a1cb-e876ef8d8579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 8.9812e-04\n",
            "Deep Learning Validation Loss: 0.0004957457422278821\n",
            "Deep Learning Validation Accuracy: 0.9999518990516663\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Learning Validation Precision: 0.0\n",
            "Deep Learning Validation Recall: 0.0\n",
            "Deep Learning Validation F1-Score: 0.0\n",
            "Deep Learning Validation Dice Coefficient: 0.0\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.7548e-04\n",
            "Deep Learning Test Loss: 0.0004672052164096385\n",
            "Deep Learning Test Accuracy: 0.9999461770057678\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
            "Deep Learning Test Precision: 0.0\n",
            "Deep Learning Test Recall: 0.0\n",
            "Deep Learning Test F1-Score: 0.0\n",
            "Deep Learning Test Dice Coefficient: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate on the validation set\n",
        "val_loss, val_accuracy = dl_model.evaluate(X_val, y_val)\n",
        "print(f\"Deep Learning Validation Loss: {val_loss}\")\n",
        "print(f\"Deep Learning Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Get predictions and convert to binary\n",
        "y_val_pred = (dl_model.predict(X_val) > 0.5).astype(int).flatten()\n",
        "y_val_true = y_val.flatten()\n",
        "\n",
        "val_precision = precision_score(y_val_true, y_val_pred)\n",
        "val_recall = recall_score(y_val_true, y_val_pred)\n",
        "val_f1 = f1_score(y_val_true, y_val_pred)\n",
        "dice_val = dice_coefficient(y_val_true, y_val_pred)\n",
        "\n",
        "print(f\"Deep Learning Validation Precision: {val_precision}\")\n",
        "print(f\"Deep Learning Validation Recall: {val_recall}\")\n",
        "print(f\"Deep Learning Validation F1-Score: {val_f1}\")\n",
        "print(f\"Deep Learning Validation Dice Coefficient: {dice_val}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = dl_model.evaluate(X_test, y_test)\n",
        "print(f\"Deep Learning Test Loss: {test_loss}\")\n",
        "print(f\"Deep Learning Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Get test predictions and convert to binary\n",
        "y_test_pred = (dl_model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "y_test_true = y_test.flatten()\n",
        "\n",
        "test_precision = precision_score(y_test_true, y_test_pred)\n",
        "test_recall = recall_score(y_test_true, y_test_pred)\n",
        "test_f1 = f1_score(y_test_true, y_test_pred)\n",
        "dice_test = dice_coefficient(y_test_true, y_test_pred)\n",
        "\n",
        "print(f\"Deep Learning Test Precision: {test_precision}\")\n",
        "print(f\"Deep Learning Test Recall: {test_recall}\")\n",
        "print(f\"Deep Learning Test F1-Score: {test_f1}\")\n",
        "print(f\"Deep Learning Test Dice Coefficient: {dice_test}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}